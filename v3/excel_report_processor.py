#!/usr/bin/env python3
import pandas as pd
import os
import argparse
import glob
import logging
from datetime import datetime
import pytz

class ExcelReportProcessor:
    """
    Helper utility to read and process Excel reports generated by the GitHub Metrics Reporter.
    Allows for extracting specific sheets, generating CSV exports, and creating summary data.
    """
    
    def __init__(self):
        """Initialize the processor with logging setup."""
        self.utc = pytz.UTC
        self._setup_logging()
        self.logger.info("Excel Report Processor initialized")
    
    def _setup_logging(self):
        """Configure logging for the processor."""
        log_dir = 'logs'
        os.makedirs(log_dir, exist_ok=True)
        
        timestamp = datetime.now(self.utc).strftime("%Y%m%d_%H%M%S")
        log_file = f'{log_dir}/excel_processor_{timestamp}.log'
        
        file_formatter = logging.Formatter(
            '%(asctime)s UTC - %(levelname)s - [%(name)s] - %(message)s',
            datefmt='%Y-%m-%d %H:%M:%S'
        )
        console_formatter = logging.Formatter('%(message)s')
        
        file_handler = logging.FileHandler(log_file, encoding='utf-8', errors='replace')
        file_handler.setFormatter(file_formatter)
        file_handler.setLevel(logging.DEBUG)
        
        console_handler = logging.StreamHandler()
        console_handler.setFormatter(console_formatter)
        console_handler.setLevel(logging.INFO)
        
        self.logger = logging.getLogger('ExcelProcessor')
        self.logger.setLevel(logging.DEBUG)
        self.logger.addHandler(file_handler)
        self.logger.addHandler(console_handler)
    
    def find_latest_report_dir(self):
        """Find the most recent report directory based on directory name pattern."""
        try:
            report_dirs = glob.glob('reports_*')
            
            if not report_dirs:
                self.logger.error("No report directories found.")
                return None
            
            # Extract timestamps from directory names
            timestamps = []
            for dir_name in report_dirs:
                # Extract timestamp from directory name (format: reports_YYYYMMDD_HHMMSS)
                if '_' in dir_name:
                    timestamp_str = dir_name.split('_', 1)[1]
                    try:
                        timestamp = datetime.strptime(timestamp_str, '%Y%m%d_%H%M%S')
                        timestamps.append((timestamp, dir_name))
                    except ValueError:
                        # Skip directories with invalid timestamp format
                        continue
            
            if not timestamps:
                self.logger.error("No valid report directories found.")
                return None
            
            # Get the most recent directory
            latest = max(timestamps, key=lambda x: x[0])
            self.logger.info(f"Found latest report directory: {latest[1]}")
            return latest[1]
            
        except Exception as e:
            self.logger.error(f"Error finding report directory: {str(e)}")
            return None
    
    def read_excel_report(self, file_path, sheet_name=None):
        """
        Read an Excel report file into a pandas DataFrame.
        
        Args:
            file_path (str): Path to the Excel file
            sheet_name (str, optional): Specific sheet to read, if None reads all sheets
            
        Returns:
            DataFrame or dict of DataFrames: The report data
        """
        try:
            self.logger.info(f"Reading Excel file: {file_path}")
            
            if not os.path.exists(file_path):
                self.logger.error(f"File not found: {file_path}")
                return None
            
            # Read the specified sheet or all sheets
            df = pd.read_excel(file_path, sheet_name=sheet_name)
            
            if isinstance(df, dict):
                for sheet, data in df.items():
                    self.logger.info(f"Sheet '{sheet}' has {len(data)} rows and {len(data.columns)} columns")
            else:
                self.logger.info(f"Data has {len(df)} rows and {len(df.columns)} columns")
            
            return df
            
        except Exception as e:
            self.logger.error(f"Error reading Excel file: {str(e)}")
            return None
    
    def export_csv(self, df, output_path):
        """
        Export DataFrame to CSV file.
        
        Args:
            df (DataFrame): The data to export
            output_path (str): Path for the output CSV file
            
        Returns:
            bool: True if successful, False otherwise
        """
        try:
            self.logger.info(f"Exporting data to CSV: {output_path}")
            
            # Create directory if it doesn't exist
            os.makedirs(os.path.dirname(output_path), exist_ok=True)
            
            # Write to CSV
            df.to_csv(output_path, index=False)
            
            self.logger.info(f"Successfully exported to {output_path}")
            return True
            
        except Exception as e:
            self.logger.error(f"Error exporting to CSV: {str(e)}")
            return False
    
    def generate_repository_summary(self, report_dir=None):
        """
        Generate a consolidated repository summary from both PR and contributor reports.
        
        Args:
            report_dir (str, optional): Directory containing the reports
            
        Returns:
            DataFrame: Consolidated summary data
        """
        try:
            # Find the latest report directory if none specified
            if not report_dir:
                report_dir = self.find_latest_report_dir()
                
            if not report_dir:
                self.logger.error("No report directory specified or found.")
                return None
            
            # Construct file paths
            pr_report_path = os.path.join(report_dir, 'pr_activity_report.xlsx')
            contributor_report_path = os.path.join(report_dir, 'contributor_report.xlsx')
            
            # Check if files exist
            if not os.path.exists(pr_report_path) or not os.path.exists(contributor_report_path):
                self.logger.error(f"Required report files not found in {report_dir}")
                return None
            
            # Read PR summary
            pr_summary = self.read_excel_report(pr_report_path, sheet_name='Repository Summary')
            if pr_summary is None:
                return None
            
            # Read PR activity to get comment and conversation counts if not in PR summary
            pr_activity = self.read_excel_report(pr_report_path, sheet_name='PR Activity')
            if pr_activity is None:
                return None
            
            # Read contributor summary
            contributor_summary = self.read_excel_report(contributor_report_path, sheet_name='Contributor Summary')
            if contributor_summary is None:
                return None
            
            # Calculate comment and conversation metrics if they don't exist in the summary
            if all(col in pr_activity.columns for col in ['Total Reviewer Comments', 'Total Approver Comments', 'Total Resolved Conversations', 'Total Unresolved Conversations']):
                if not all(col in pr_summary.columns for col in ['Total Reviewer Comments', 'Total Approver Comments', 'Total Resolved Conversations', 'Total Unresolved Conversations']):
                    # Aggregate from PR activity
                    comment_metrics = pr_activity.groupby('Repository').agg({
                        'Total Reviewer Comments': 'sum',
                        'Total Approver Comments': 'sum',
                        'Total Resolved Conversations': 'sum',
                        'Total Unresolved Conversations': 'sum'
                    }).reset_index()
                    
                    # Merge with PR summary
                    pr_summary = pd.merge(pr_summary, comment_metrics, on='Repository', how='left')
                    pr_summary.fillna(0, inplace=True)
            
            # Calculate breaking change counts if they don't exist in the summary
            if 'Is Breaking Change' in pr_activity.columns and 'Breaking Change PRs' not in pr_summary.columns:
                breaking_changes = pr_activity[pr_activity['Is Breaking Change'] == 'Yes'].groupby('Repository').size().reset_index(name='Breaking Change PRs')
                pr_summary = pd.merge(pr_summary, breaking_changes, on='Repository', how='left')
                pr_summary['Breaking Change PRs'].fillna(0, inplace=True)
            
            # Generate consolidated summary
            summary_data = {
                'Total Repositories': len(pr_summary),
                'Total Contributors': len(contributor_summary),
                'Total PRs': pr_summary['Total PRs'].sum(),
                'Merged PRs': pr_summary['Merged PRs'].sum(),
                'Open PRs': pr_summary['Total PRs'].sum() - pr_summary['Merged PRs'].sum(),
                'Healthy PRs': pr_summary['Healthy PRs'].sum(),
                'Unhealthy PRs': pr_summary['Unhealthy PRs'].sum(),
                'Health Percentage': round(pr_summary['Healthy PRs'].sum() / pr_summary['Total PRs'].sum() * 100, 1) if pr_summary['Total PRs'].sum() > 0 else 0,
                'RC Versions': pr_summary['RC Versions'].sum(),
                'NPD Versions': pr_summary['NPD Versions'].sum(),
                'Stable Versions': pr_summary['Stable Versions'].sum(),
                'Avg PR Duration (days)': round(pr_summary['Avg PR Duration (days)'].mean(), 1),
                'Total Commits': contributor_summary['Total Commits'].sum(),
                'Avg Commits per Contributor': round(contributor_summary['Total Commits'].sum() / len(contributor_summary) if len(contributor_summary) > 0 else 0, 1)
            }
            
            # Add comment metrics if available
            if 'Total Reviewer Comments' in pr_summary.columns:
                summary_data['Total Reviewer Comments'] = pr_summary['Total Reviewer Comments'].sum()
            
            if 'Total Approver Comments' in pr_summary.columns:
                summary_data['Total Approver Comments'] = pr_summary['Total Approver Comments'].sum()
            
            if 'Total Resolved Conversations' in pr_summary.columns:
                summary_data['Total Resolved Conversations'] = pr_summary['Total Resolved Conversations'].sum()
            
            if 'Total Unresolved Conversations' in pr_summary.columns:
                summary_data['Total Unresolved Conversations'] = pr_summary['Total Unresolved Conversations'].sum()
            
            # Add breaking change counts if available
            if 'Breaking Change PRs' in pr_summary.columns:
                summary_data['Breaking Change PRs'] = pr_summary['Breaking Change PRs'].sum()
            
            # Add new contributor metrics if available
            new_contributor_metrics = [
                'Total Commit Passed Checks',
                'Total Commit Failed Checks',
                'Feature/Fix PRs',
                'With Examples',
                'With Tests',
                'With Integration Tests'
            ]
            
            for metric in new_contributor_metrics:
                if metric in contributor_summary.columns:
                    summary_data[metric] = contributor_summary[metric].sum()
            
            # Create DataFrame from summary data
            summary_df = pd.DataFrame([summary_data])
            
            # Generate output path for summary
            summary_path = os.path.join(report_dir, 'consolidated_summary.csv')
            self.export_csv(summary_df, summary_path)
            
            self.logger.info(f"Generated consolidated summary in {summary_path}")
            return summary_df
            
        except Exception as e:
            self.logger.error(f"Error generating repository summary: {str(e)}")
            return None
    
    def extract_top_contributors(self, report_dir=None, top_n=10):
        """
        Extract the top contributors by PR count.
        
        Args:
            report_dir (str, optional): Directory containing the reports
            top_n (int): Number of top contributors to extract
            
        Returns:
            DataFrame: Top contributors data
        """
        try:
            # Find the latest report directory if none specified
            if not report_dir:
                report_dir = self.find_latest_report_dir()
                
            if not report_dir:
                self.logger.error("No report directory specified or found.")
                return None
            
            # Construct file path
            contributor_report_path = os.path.join(report_dir, 'contributor_report.xlsx')
            
            # Check if file exists
            if not os.path.exists(contributor_report_path):
                self.logger.error(f"Contributor report not found in {report_dir}")
                return None
            
            # Read contributor summary
            contributor_summary = self.read_excel_report(contributor_report_path, sheet_name='Contributor Summary')
            if contributor_summary is None:
                return None
            
            # Sort and get top contributors
            top_contributors = contributor_summary.sort_values(by='Total PRs', ascending=False).head(top_n)
            
            # Generate output path for top contributors
            output_path = os.path.join(report_dir, f'top_{top_n}_contributors.csv')
            self.export_csv(top_contributors, output_path)
            
            self.logger.info(f"Extracted top {top_n} contributors to {output_path}")
            return top_contributors
            
        except Exception as e:
            self.logger.error(f"Error extracting top contributors: {str(e)}")
            return None