# GitHub Metrics Dashboard

## Overview

The GitHub Metrics Dashboard is a visualization tool for GitHub repository metrics that provides insights into repository health, contributor performance, and pull request activity. This dashboard works with Excel reports generated by the GitHub Metrics Reporter script.

## Features

- **Overall Summary**: High-level metrics across all repositories
- **Contributor Analysis**: Individual and team contribution metrics, including review activities
- **PR Activity Analysis**: Pull request health and processing metrics
- **Support for Multiple Report Locations**: Automatic detection of report files
- **Interactive Data Tables**: Sortable tables with detailed metrics
- **Repository Status Tracking**: Identification of active repositories vs. stable/no-development repositories
- **Comprehensive Contributor Tracking**: Metrics for all contributors, even those without recent activity
- **Comment Analysis**: Detailed tracking of reviewer and approver comments with resolution metrics
- **Breaking Change Detection**: Identification and analysis of breaking change PRs
- **Enhanced Commit Tracking**: Capture all commits within the date range, even from contributors who didn't create PRs
- **Proper Pagination Handling**: Complete capture of commit data for PRs of any size

## Installation

### Prerequisites

- Python 3.7 or higher
- Required Python packages (install using `pip install -r requirements.txt`):
  - streamlit
  - pandas
  - openpyxl
  - pytz
  - python-dateutil
  - matplotlib

### Setup

1. Clone this repository or download the source files
2. Install the required dependencies:
   ```
   pip install -r requirements.txt
   ```

## Usage

### Generating Metrics Reports

First, run the GitHub Metrics Reporter to generate the Excel reports:

```
python gh_metrics_enhanced.py --org <ORGANIZATION_NAME> --start-date <YYYY-MM-DD> --end-date <YYYY-MM-DD> [OPTIONS]
```

#### Required Parameters:
- `--org`: GitHub organization/owner name
- `--start-date`: Start date for analysis in YYYY-MM-DD format
- `--end-date`: End date for analysis in YYYY-MM-DD format

#### Optional Parameters:
- `--repos-file`: Path to file containing repository names (one per line)
- `--token-file`: Path to file containing GitHub token (alternatively, set GITHUB_TOKEN environment variable)
- `--output-dir`: Custom output directory path (default: reports_YYYYMMDD_HHMMSS)
- `--pr-threshold`: PR health threshold in days (default: 7)
- `--label-threshold`: Maximum labels threshold (default: 2)

Example:
```
python gh_metrics_enhanced.py --org my-organization --start-date 2023-01-01 --end-date 2023-01-31 --token-file github_token.txt --output-dir my_reports
```

This will generate Excel reports in the specified output directory.

### Processing Reports (Optional)

You can use the Excel Report Processor to generate additional summary data or extract specific information:

```
# To generate a consolidated summary from the latest reports
python -c "from excel_report_processor import ExcelReportProcessor; processor = ExcelReportProcessor(); processor.generate_repository_summary()"

# To extract top contributors from a specific report directory
python -c "from excel_report_processor import ExcelReportProcessor; processor = ExcelReportProcessor(); processor.extract_top_contributors('path/to/reports', top_n=10)"

# To read and export a specific sheet from a report
python -c "from excel_report_processor import ExcelReportProcessor; processor = ExcelReportProcessor(); data = processor.read_excel_report('path/to/report.xlsx', 'Sheet Name'); processor.export_csv(data, 'output.csv')"
```

### Viewing Reports in the Dashboard

1. Run the dashboard:
   ```
   streamlit run github_metrics_dashboard.py
   ```
2. The dashboard will automatically open in your default web browser
3. Select the report directory from the sidebar or specify a custom location
4. Navigate between the three dashboard tabs to explore different metrics


1. Run the dashboard:
   ```
   streamlit run github_metrics_dashboard.py
   ```
2. The dashboard will automatically open in your default web browser
3. Select the report directory from the sidebar or specify a custom location
4. Navigate between the three dashboard tabs to explore different metrics

## Components

The GitHub Metrics Dashboard consists of three main components:

1. **GitHub Metrics Reporter (`gh_metrics_enhanced.py`)**: This script connects to the GitHub API, collects repository and contributor metrics, and generates Excel reports.
2. **Excel Report Processor (`excel_report_processor.py`)**: A utility that reads and processes the Excel reports generated by the GitHub Metrics Reporter.
3. **GitHub Metrics Dashboard (`github_metrics_dashboard.py`)**: A Streamlit web application that visualizes the data from the Excel reports.

## Recent Enhancements

### Improved PR Commit Tracking

The latest version of the GitHub Metrics Reporter implements proper pagination when fetching commits for pull requests. This ensures that the total commit count in the reports matches what you see in the GitHub UI, even for pull requests with a large number of commits (over 100).

### Enhanced Contributor Tracking

The contributor reporting has been enhanced to capture all contributors who made commits within the specified date range, even if they didn't create any PRs during that period. This provides a more complete picture of project activity by including:

- Contributors who only made commits to PRs that were created before the start date
- Contributors who made direct commits without PRs
- All commit activity within the date range, regardless of PR creation date

This enhancement ensures that all code contributions are properly attributed and counted, giving a more accurate representation of contributor activity.

## Metrics and Calculations

The dashboard uses several key metrics to evaluate repository and contributor performance. Below are the most important calculations used throughout the dashboard. All of these metrics are derived directly from data in the Excel reports and calculated within the dashboard script.

### PR Health Score

The PR Health Score is a measure of pull request quality and efficiency. A PR is considered "healthy" if it meets both of these criteria:

1. It has been open for less than or equal to 7 days (configurable)
2. It has 2 or fewer labels (configurable)

PRs that don't meet these criteria are marked as "Needs Attention".

The Health Score calculation is:

```
Health Percentage = (Healthy PRs / Total PRs) * 100
```

The Health Status is determined by the Health Percentage:
- ðŸŸ¢: >= 80% (Good)
- ðŸŸ¡: >= 50% and < 80% (Moderate)
- ðŸ”´: < 50% (Poor)
- âšª: No PRs in the date range (Stable/No Dev)

#### Example:
If a repository has 20 total PRs with 16 healthy PRs:
- Health Percentage = (16 / 20) * 100 = 80%
- Health Status: ðŸŸ¢

If a repository has no PRs in the analyzed period:
- Health Percentage = 100% (default)
- Health Status: âšª (indicating Stable/No Dev)

### Repository Status Classification

Repositories are classified into different status categories:

- **Active Development**: Repositories with PRs in the analyzed period
- **Stable/No Development**: Repositories with no PRs in the analyzed period

This classification helps teams focus attention on repositories with active development while identifying codebases that are stable or not currently under development.

### PRs Approved by Contributor

This metric counts the number of pull requests that each contributor has approved as a reviewer. It helps identify contributors who actively participate in code review processes.

#### Calculation:
The dashboard counts the number of times a contributor's name appears in the "Approvers" list across all PRs.

#### Example:
If contributor "JaneDoe" appears as an approver in 12 different PRs:
- PRs Approved by JaneDoe = 12

### Change Requests Given by Contributor

This metric tracks how many change requests each contributor has submitted during code reviews. It helps identify contributors who actively enforce code quality standards.

#### Calculation:
The dashboard counts the number of times a contributor is recorded as having requested changes to a PR.

#### Example:
If contributor "JohnSmith" has requested changes in 8 different PRs:
- Change Requests Given by JohnSmith = 8

### PRs with Comments Percentage

This metric measures how many PRs receive meaningful review comments, which is an important indicator of code review quality:

```
PRs with Comments Percentage = (PRs With Comments / (PRs With Comments + PRs Without Comments)) * 100
```

#### Example:
If 30 PRs received comments during review and 10 PRs were approved without comments:
- PRs with Comments Percentage = (30 / (30 + 10)) * 100 = 75%

### Comment Activity Metrics

These metrics track the depth and quality of review activity through comment analysis.

#### Reviewer and Approver Comments

The dashboard distinguishes between comments from reviewers and comments from approvers:

- **Reviewer Comments**: Comments provided by team members who have not approved the PR
- **Approver Comments**: Comments provided by team members who have approved the PR

#### Calculation:
```
Total Reviewer Comments = Sum of all comments from non-approvers
Total Approver Comments = Sum of all comments from approvers
```

#### Example:
If a PR received 8 comments from reviewers who didn't approve the PR and 5 comments from team members who eventually approved the PR:
- Total Reviewer Comments = 8
- Total Approver Comments = 5

#### Conversation Resolution Metrics

These metrics measure how effectively review conversations are being resolved:

```
Total Resolved Conversations = Count of review threads where the PR author has responded
Total Unresolved Conversations = Count of review threads where the PR author has not responded
Resolution Rate = (Resolved Conversations / Total Conversations) * 100
```

#### Example:
If a repository has 45 resolved conversations and 15 unresolved conversations:
- Resolution Rate = (45 / 60) * 100 = 75%

This indicates that 75% of all review conversations have been addressed by the PR author, which is a good indicator of responsiveness to feedback.

### Breaking Change Detection

This metric identifies PRs that introduce breaking changes, which is important for versioning and release planning.

#### Calculation:
A PR is marked as a breaking change if its title contains 'feat!' at the beginning, indicating a feature change that breaks backward compatibility.

```
Breaking Change PRs = Count of PRs with 'feat!' prefix in title
Breaking Change Percentage = (Breaking Change PRs / Total PRs) * 100
```

#### Example:
If a repository has 100 total PRs and 5 of them have titles starting with 'feat!':
- Breaking Change PRs = 5
- Breaking Change Percentage = (5 / 100) * 100 = 5%

This repository has a 5% breaking change rate, which might indicate significant architectural changes or interface updates.

### Check Success Rate

The Check Success Rate measures the reliability of code in passing automated checks and tests:

```
Check Success Rate = (Passed Checks / (Passed Checks + Failed Checks)) * 100
```

#### Example:
If a repository has 150 passed checks and 30 failed checks:
- Check Success Rate = (150 / (150 + 30)) * 100 = 83.3%

### Average Calculations

The dashboard employs several averaging calculations to provide meaningful metrics across different dimensions:

#### Average PR Duration (Days)

This calculates the mean time PRs remain open:

```
Average PR Duration = Sum of PR Duration for All PRs / Total Number of PRs
```

#### Example:
If 5 PRs were open for 2, 4, 7, 3, and 9 days respectively:
- Average PR Duration = (2 + 4 + 7 + 3 + 9) / 5 = 5 days

#### Average Comments per PR

This measures the typical level of review discussion for each PR:

```
Average Comments per PR = (Total Reviewer Comments + Total Approver Comments) / Total Number of PRs
```

#### Example:
If there are 200 reviewer comments, 150 approver comments, and 50 total PRs:
- Average Comments per PR = (200 + 150) / 50 = 7 comments per PR

This metric helps identify whether PRs are receiving sufficient review attention.

#### Average Commits per Contributor

This measures the typical contribution volume per contributor:

```
Average Commits per Contributor = Total Commits Across All Repositories / Total Number of Contributors
```

#### Example:
If there are 800 total commits and 20 contributors:
- Average Commits per Contributor = 800 / 20 = 40 commits

#### Average PRs per Contributor

Similar to commits, this measures the typical PR creation rate:

```
Average PRs per Contributor = Total PRs Across All Repositories / Total Number of Contributors
```

#### Example:
If there are 120 total PRs and 20 contributors:
- Average PRs per Contributor = 120 / 20 = 6 PRs

#### Average PRs Approved per Contributor

This metric measures the typical code review participation rate:

```
Average PRs Approved per Contributor = Total PRs Approved Across All Contributors / Total Number of Contributors
```

#### Example:
If there are 200 total PR approvals and 20 contributors:
- Average PRs Approved per Contributor = 200 / 20 = 10 approvals

#### Average Change Requests per Contributor

This metric measures the typical code quality enforcement rate:

```
Average Change Requests per Contributor = Total Change Requests Given Across All Contributors / Total Number of Contributors
```

#### Example:
If there are 100 total change requests and 20 contributors:
- Average Change Requests per Contributor = 100 / 20 = 5 change requests

### Version Type Metrics

The dashboard analyzes version labels to categorize pull requests into different release types:

- **RC Versions**: Labels ending with '-rc' (Release Candidate)
- **NPD Versions**: Labels ending with '-npd' (Non-Production Deployment)
- **Stable Versions**: All other version labels

The distribution of these version types helps track the release cycle maturity across repositories.

### PR Age Distribution

PRs are categorized by age to analyze workflow efficiency:

- 1 day or less
- 1-3 days
- 3-7 days
- 7-14 days
- 14-30 days
- Over 30 days

This distribution helps identify bottlenecks in the review process. PRs that exceed the configurable threshold (default: 7 days) are marked as "Needs Attention".

### Contributor Performance Metrics

For each contributor, the dashboard calculates:

1. **Individual Health Score**: The percentage of a contributor's PRs that are healthy.
2. **Check Success Rate**: The success rate of all checks in a contributor's PRs.
3. **Multi-Repo Contribution**: The number of repositories a contributor is active in.
4. **Review Activity**: The number of PRs approved and change requests given by the contributor.
5. **Comment Activity**: The number of comments provided as a reviewer and as an approver.
6. **Breaking Change Rate**: The percentage of a contributor's PRs that introduce breaking changes.
7. **Total Commits**: The total number of commits made by the contributor within the date range, including commits to PRs created outside the date range.

The dashboard tracks all contributors, even those with no activity in the analyzed period, providing a comprehensive view of the contributor ecosystem.

### Contributor Activity Status

Contributors are classified with activity indicators:
- ðŸŸ¢: High health score (â‰¥80%) with active contributions
- ðŸŸ¡: Medium health score (â‰¥50%, <80%) with active contributions
- ðŸ”´: Low health score (<50%) with active contributions
- âšª: No activity in the analyzed period

### Multi-Repo Contributors Percentage

This metric shows what percentage of all contributors work across multiple repositories:

```
Multi-Repo Contributors Percentage = (Multi-Repo Contributors / Total Contributors) * 100
```

Where a Multi-Repo Contributor is defined as someone who has contributed to two or more repositories.

#### Example:
If there are 25 total contributors and 10 of them have contributed to multiple repositories:
- Multi-Repo Contributors Percentage = (10 / 25) * 100 = 40%

## Review Activity Metrics

The dashboard includes dedicated metrics to analyze code review participation:

### Total PRs Approved

This counts the total number of PR approvals across all contributors, providing insight into the overall review volume.

### Total Change Requests Given

This counts the total number of change requests submitted across all contributors, indicating the level of quality enforcement.

### Reviewer to Author Ratio

This metric helps evaluate if there is a healthy balance between code authorship and review participation:

```
Reviewer to Author Ratio = (Total PRs Approved + Total Change Requests Given) / Total PRs Created
```

#### Example:
If there are 300 PRs created, 250 PRs approved, and 100 change requests given:
- Reviewer to Author Ratio = (250 + 100) / 300 = 1.17

A ratio greater than 1.0 generally indicates a healthy review culture where there is more review activity than code authorship.

### Comment Distribution Analysis

This metric evaluates the distribution of comments between reviewers and approvers:

```
Reviewer Comment Percentage = (Total Reviewer Comments / (Total Reviewer Comments + Total Approver Comments)) * 100
Approver Comment Percentage = (Total Approver Comments / (Total Reviewer Comments + Total Approver Comments)) * 100
```

#### Example:
If there are 300 reviewer comments and 200 approver comments:
- Reviewer Comment Percentage = (300 / 500) * 100 = 60%
- Approver Comment Percentage = (200 / 500) * 100 = 40%

This distribution helps evaluate whether most feedback is coming before approval (reviewers) or along with approval (approvers).

## FAQ

### How are PR health thresholds determined?

The default thresholds (7 days for duration, 2 for label count) are configurable in the GitHub Metrics Reporter script. They represent typical industry standards for efficient PR processing but can be adjusted based on your team's workflow.

### What does a low health score indicate?

A low health score could indicate several issues:
- PRs staying open too long without resolution
- Excessive use of labels, suggesting complex or poorly scoped PRs
- Process bottlenecks in the review workflow

### What does the "Stable/No Dev" status mean?

Repositories marked as "Stable/No Dev" had no PRs within the analyzed time period. This could indicate:
- A stable codebase that doesn't require active development
- A repository that is in maintenance mode
- A new repository that hasn't begun active development
- A deprecated repository that is no longer being developed

### How are contributors with no activity handled?

The dashboard tracks all known contributors, even if they have no activity in the analyzed period. This provides:
- A complete picture of the contributor ecosystem
- Visibility into contributor churn or inactivity
- Historical context when contributors become active again

### What constitutes a "resolved" conversation?

A conversation is considered resolved when the PR author has responded to the thread. This simple heuristic helps measure whether feedback is being acknowledged and addressed.

### How are breaking changes identified?

Breaking changes are identified by the 'feat!' prefix in the PR title, following conventional commit standards. This prefix indicates a feature change that breaks backward compatibility.

### How does the system track contributors who didn't create PRs?

The enhanced contributor tracking system fetches all commits within the specified date range directly from the GitHub API, regardless of whether they are part of a PR. This ensures that all coding activity is captured, even from contributors who:
- Only commit to PRs created outside the date range
- Make direct commits to repositories without creating PRs
- Contribute as co-authors or reviewers without being the PR creator

### Why might the PR commit count in GitHub UI differ from the reports?

Prior to the pagination enhancement, the GitHub Metrics Reporter could only fetch up to 100 commits per PR, causing discrepancies for PRs with more than 100 commits. The improved implementation now properly handles pagination to fetch all commits, ensuring that the count in the reports matches what you see in the GitHub UI.

### How can we improve our metrics?

1. **Improve PR Health Score**:
   - Encourage smaller, well-scoped PRs
   - Implement a PR review SLA
   - Address PRs marked as "Needs Attention" promptly
   - Use standard label sets consistently

2. **Improve Check Success Rate**:
   - Address failed checks promptly
   - Improve test coverage and reliability
   - Conduct pre-commit testing

3. **Contributor Performance**:
   - Provide feedback based on individual metrics
   - Recognize high-performing contributors
   - Support contributors with lower metrics through mentoring

4. **Improve Review Activity**:
   - Establish review rotation schedules to distribute the review load
   - Set expectations for minimum review participation
   - Recognize contributors who provide thorough and helpful reviews

5. **Enhance Comment Quality**:
   - Encourage substantive, actionable feedback
   - Promote explicit resolutions for all comment threads
   - Balance reviewer and approver comment distribution

6. **Manage Breaking Changes**:
   - Clearly document all breaking changes
   - Schedule breaking changes strategically in release planning
   - Provide migration paths for consumers of breaking changes

## Troubleshooting

If the dashboard cannot find your reports:
1. Check that the reports are in one of the expected locations
2. Use the "Specify custom directory" option to manually enter the path
3. Verify that your Excel files contain the expected sheet names

If metrics appear incorrect:
1. Verify the original data in the Excel reports
2. Check for any unusual formatting or data types in the Excel files
3. Look for specific error messages in the dashboard for guidance

If report generation is taking longer than expected:
1. Check network connectivity to the GitHub API
2. Consider narrowing the date range for analysis
3. For large repositories, the enhanced contributor tracking may take longer due to fetching all commits within the date range

## License

This project is licensed under the MIT License - see the LICENSE file for details.

## Acknowledgments

- GitHub API for providing the underlying data
- Streamlit for the interactive dashboard framework
- Pandas for data processing capabilities